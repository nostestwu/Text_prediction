{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e6cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57155bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mowgli’s brothers\n",
      "\n",
      "     now rann the kite brings home the night\n",
      "        that mang the bat sets free--\n",
      "     the herds are shut in byre and hut\n",
      "        for loosed till dawn are we.\n",
      "     this is the hour of pride and power,\n",
      "        talon and tush and claw.\n",
      "     oh, hear the call!--good hunting all\n",
      "        that keep the jungle law!\n",
      "     night-song in the jungle\n",
      "\n",
      "it was seven o’clock of a very warm evening in the seeonee hills when\n",
      "father wolf woke up from his day’s rest, scratched himself, yawned, and\n",
      "spread out his paws one after the other to get rid of the sleepy feeling\n",
      "in their tips. mother wolf lay with her big gray nose dropped across her\n",
      "four tumbling, squealing cubs, and the moon shone into the mouth of the\n",
      "cave where they all lived. “augrh!” said father wolf. “it is time to\n",
      "hunt again.” he was going to spring down hill when a little shadow with\n",
      "a bushy tail crossed the threshold and whined: “good luck go with you, o\n",
      "chief of the wolves. and good luck and strong white teeth go with\n"
     ]
    }
   ],
   "source": [
    "#LOAD TEXT\n",
    "#Save notepad as UTF-8 (select from dropdown during saving)\n",
    "filename = \"data/book.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "print(raw_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e6539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TEXT\n",
    "#Remove numbers\n",
    "raw_text = ''.join(c for c in raw_text if not c.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e015354b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pk/zmnwmmlj6d16wn3l4ql40ym80000gn/T/ipykernel_84071/3713822886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#How many total characters do we have in our training text?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#List of every character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Character sequences must be encoded as integers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shape' is not defined"
     ]
    }
   ],
   "source": [
    "#How many total characters do we have in our training text?\n",
    "chars = sorted(list(set(raw_text))) #List of every character\n",
    "\n",
    "\n",
    "#Character sequences must be encoded as integers. \n",
    "#Each unique character will be assigned an integer value. \n",
    "#Create a dictionary of characters mapped to integer values\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "#Do the reverse so we can print our predictions in characters and not integers\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017aa702",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#Now that we have characters we can create input/output sequences for training\n",
    "#Remember that for LSTM input and output can be sequences... hence the term seq2seq\n",
    "\n",
    "\n",
    "seq_length = 60  #Length of each input sequence\n",
    "step = 10   #Instead of moving 1 letter at a time, try skipping a few. \n",
    "sentences = []    # X values (Sentences)\n",
    "next_chars = []   # Y values. The character that follows the sentence defined as X\n",
    "for i in range(0, n_chars - seq_length, step):  #step=1 means each sentence is offset just by a single letter\n",
    "    sentences.append(raw_text[i: i + seq_length])  #Sequence in\n",
    "    next_chars.append(raw_text[i + seq_length])  #Sequence out\n",
    "n_patterns = len(sentences)    \n",
    "print('Number of sequences:', n_patterns)\n",
    "\n",
    "#Have a look at sentences and next_chars to see the continuity...\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ab01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just like time series, X is the sequence / sentence and y is the next value\n",
    "#that comes after the sentence... \n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "\n",
    "#time steps = sequence length\n",
    "#features = numbers of characters in our vocab (n_vocab)\n",
    "#Vectorize all sentences: there are n_patterns sentences.\n",
    "#For each sentence we have n_vocab characters available for seq_length\n",
    "#Vectorization returns a vector for all sentences indicating the presence or absence \n",
    "#of a character. \n",
    "\n",
    "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1\n",
    "    \n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444856c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic model with one LSTM\n",
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6789d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper model woth 2 LSTM\n",
    "#To stack LSTM layers, we need to change the configuration of the prior \n",
    "#LSTM layer to output a 3D array as input for the subsequent layer.\n",
    "#We can do this by setting the return_sequences argument on the layer to True \n",
    "#(defaults to False). This will return one output for each input time step and provide a 3D array.\n",
    "#Below is the same example as above with return_sequences=True.\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(128, input_shape=(seq_length, n_vocab), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(128))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47908b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,   \n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "model.save('my_saved_weights_jungle_book_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b45687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate characters \n",
    "#We must provide a sequence of seq_lenth as input to start the generation process\n",
    "\n",
    "#The prediction results is probabilities for each of the 48 characters at a specific\n",
    "#point in sequence. Let us pick the one with max probability and print it out.\n",
    "#Writing our own softmax function....\n",
    "\n",
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "# load the network weights\n",
    "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
    "model.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a753013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9599b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
    "#sys.stdout.write(generated)\n",
    "\n",
    "\n",
    "for i in range(400):   # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762a9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb0c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e2633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "230.852px",
    "left": "391px",
    "right": "20px",
    "top": "120px",
    "width": "380px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
