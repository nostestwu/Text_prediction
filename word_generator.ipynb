{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e6cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57155bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mowgli’s brothers\n",
      "\n",
      "     now rann the kite brings home the night\n",
      "        that mang the bat sets free--\n",
      "     the herds are shut in byre and hut\n",
      "        for loosed till dawn are we.\n",
      "     this is the hour of pride and power,\n",
      "        talon and tush and claw.\n",
      "     oh, hear the call!--good hunting all\n",
      "        that keep the jungle law!\n",
      "     night-song in the jungle\n",
      "\n",
      "it was seven o’clock of a very warm evening in the seeonee hills when\n",
      "father wolf woke up from his day’s rest, scratched himself, yawned, and\n",
      "spread out his paws one after the other to get rid of the sleepy feeling\n",
      "in their tips. mother wolf lay with her big gray nose dropped across her\n",
      "four tumbling, squealing cubs, and the moon shone into the mouth of the\n",
      "cave where they all lived. “augrh!” said father wolf. “it is time to\n",
      "hunt again.” he was going to spring down hill when a little shadow with\n",
      "a bushy tail crossed the threshold and whined: “good luck go with you, o\n",
      "chief of the wolves. and good luck and strong white teeth go with\n"
     ]
    }
   ],
   "source": [
    "#LOAD TEXT\n",
    "#Save notepad as UTF-8 (select from dropdown during saving)\n",
    "filename = \"data/book.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "print(raw_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e6539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TEXT\n",
    "#Remove digits\n",
    "raw_text = ''.join(c for c in raw_text if not c.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e015354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many total characters do we have in our training text?\n",
    "chars = sorted(list(set(raw_text))) #List of every character\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1bde73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in the text; corpus length:  291857\n",
      "Total Vocab:  49\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017aa702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 29180\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "#Now that we have characters we can create input/output sequences for training\n",
    "#Remember that for LSTM input and output can be sequences... hence the term seq2seq\n",
    "\n",
    "\n",
    "seq_length = 60  #Length of each input sequence\n",
    "step = 10   #Instead of moving 1 letter at a time, try skipping a few. \n",
    "sentences = []    # X values (Sentences)\n",
    "next_chars = []   # Y values. The character that follows the sentence defined as X\n",
    "for i in range(0, n_chars - seq_length, step):  #step=1 means each sentence is offset just by a single letter\n",
    "    sentences.append(raw_text[i: i + seq_length])  #Sequence in\n",
    "    next_chars.append(raw_text[i + seq_length])  #Sequence out\n",
    "n_patterns = len(sentences)    \n",
    "print('Number of sequences:', n_patterns)\n",
    "\n",
    "#Have a look at sentences and next_chars to see the continuity...\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080ab01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/zmnwmmlj6d16wn3l4ql40ym80000gn/T/ipykernel_99388/1466857379.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
      "/var/folders/pk/zmnwmmlj6d16wn3l4ql40ym80000gn/T/ipykernel_99388/1466857379.py:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29180, 60, 49)\n",
      "(29180, 49)\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False  True False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]]\n"
     ]
    }
   ],
   "source": [
    "#Just like time series, X is the sequence / sentence and y is the next value\n",
    "#that comes after the sentence... \n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "\n",
    "#time steps = sequence length\n",
    "#features = numbers of characters in our vocab (n_vocab)\n",
    "#Vectorize all sentences: there are n_patterns sentences.\n",
    "#For each sentence we have n_vocab characters available for seq_length\n",
    "#Vectorization returns a vector for all sentences indicating the presence or absence \n",
    "#of a character. \n",
    "\n",
    "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1\n",
    "    \n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444856c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:37:10.564125: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               91136     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 49)                6321      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,457\n",
      "Trainable params: 97,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nostest/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Basic model with one LSTM\n",
    "# build the model: a single LSTM\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "#model.add(Dense(n_vocab, activation='softmax'))\n",
    "#\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b6789d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 60, 128)           91136     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 128)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 49)                6321      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 229,041\n",
      "Trainable params: 229,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nostest/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Deeper model woth 2 LSTM\n",
    "#To stack LSTM layers, we need to change the configuration of the prior \n",
    "#LSTM layer to output a 3D array as input for the subsequent layer.\n",
    "#We can do this by setting the return_sequences argument on the layer to True \n",
    "#(defaults to False). This will return one output for each input time step and provide a 3D array.\n",
    "#Below is the same example as above with return_sequences=True.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d47908b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.7295\n",
      "Epoch 1: loss improved from inf to 2.72948, saving model to saved_weights/saved_weights-01-2.7295.hdf5\n",
      "228/228 [==============================] - 39s 160ms/step - loss: 2.7295\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.1584\n",
      "Epoch 2: loss improved from 2.72948 to 2.15836, saving model to saved_weights/saved_weights-02-2.1584.hdf5\n",
      "228/228 [==============================] - 36s 158ms/step - loss: 2.1584\n",
      "Epoch 3/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.9636\n",
      "Epoch 3: loss improved from 2.15836 to 1.96359, saving model to saved_weights/saved_weights-03-1.9636.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.9636\n",
      "Epoch 4/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.8385\n",
      "Epoch 4: loss improved from 1.96359 to 1.83855, saving model to saved_weights/saved_weights-04-1.8385.hdf5\n",
      "228/228 [==============================] - 36s 157ms/step - loss: 1.8385\n",
      "Epoch 5/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.7388\n",
      "Epoch 5: loss improved from 1.83855 to 1.73885, saving model to saved_weights/saved_weights-05-1.7388.hdf5\n",
      "228/228 [==============================] - 36s 158ms/step - loss: 1.7388\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.6570\n",
      "Epoch 6: loss improved from 1.73885 to 1.65700, saving model to saved_weights/saved_weights-06-1.6570.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.6570\n",
      "Epoch 7/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.5909\n",
      "Epoch 7: loss improved from 1.65700 to 1.59085, saving model to saved_weights/saved_weights-07-1.5909.hdf5\n",
      "228/228 [==============================] - 36s 158ms/step - loss: 1.5909\n",
      "Epoch 8/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.5405\n",
      "Epoch 8: loss improved from 1.59085 to 1.54051, saving model to saved_weights/saved_weights-08-1.5405.hdf5\n",
      "228/228 [==============================] - 36s 160ms/step - loss: 1.5405\n",
      "Epoch 9/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4922\n",
      "Epoch 9: loss improved from 1.54051 to 1.49221, saving model to saved_weights/saved_weights-09-1.4922.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.4922\n",
      "Epoch 10/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 10: loss improved from 1.49221 to 1.45550, saving model to saved_weights/saved_weights-10-1.4555.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.4555\n",
      "Epoch 11/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 11: loss improved from 1.45550 to 1.41540, saving model to saved_weights/saved_weights-11-1.4154.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.4154\n",
      "Epoch 12/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.3886\n",
      "Epoch 12: loss improved from 1.41540 to 1.38860, saving model to saved_weights/saved_weights-12-1.3886.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.3886\n",
      "Epoch 13/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.3560\n",
      "Epoch 13: loss improved from 1.38860 to 1.35599, saving model to saved_weights/saved_weights-13-1.3560.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.3560\n",
      "Epoch 14/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.3271\n",
      "Epoch 14: loss improved from 1.35599 to 1.32712, saving model to saved_weights/saved_weights-14-1.3271.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.3271\n",
      "Epoch 15/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.3053\n",
      "Epoch 15: loss improved from 1.32712 to 1.30528, saving model to saved_weights/saved_weights-15-1.3053.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.3053\n",
      "Epoch 16/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2887\n",
      "Epoch 16: loss improved from 1.30528 to 1.28873, saving model to saved_weights/saved_weights-16-1.2887.hdf5\n",
      "228/228 [==============================] - 36s 160ms/step - loss: 1.2887\n",
      "Epoch 17/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2801\n",
      "Epoch 17: loss improved from 1.28873 to 1.28008, saving model to saved_weights/saved_weights-17-1.2801.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.2801\n",
      "Epoch 18/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2523\n",
      "Epoch 18: loss improved from 1.28008 to 1.25233, saving model to saved_weights/saved_weights-18-1.2523.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.2523\n",
      "Epoch 19/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2421\n",
      "Epoch 19: loss improved from 1.25233 to 1.24213, saving model to saved_weights/saved_weights-19-1.2421.hdf5\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.2421\n",
      "Epoch 20/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2285\n",
      "Epoch 20: loss improved from 1.24213 to 1.22850, saving model to saved_weights/saved_weights-20-1.2285.hdf5\n",
      "228/228 [==============================] - 36s 160ms/step - loss: 1.2285\n",
      "Epoch 21/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2229\n",
      "Epoch 21: loss improved from 1.22850 to 1.22294, saving model to saved_weights/saved_weights-21-1.2229.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.2229\n",
      "Epoch 22/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2018\n",
      "Epoch 22: loss improved from 1.22294 to 1.20179, saving model to saved_weights/saved_weights-22-1.2018.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.2018\n",
      "Epoch 23/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1927\n",
      "Epoch 23: loss improved from 1.20179 to 1.19268, saving model to saved_weights/saved_weights-23-1.1927.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.1927\n",
      "Epoch 24/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1769\n",
      "Epoch 24: loss improved from 1.19268 to 1.17691, saving model to saved_weights/saved_weights-24-1.1769.hdf5\n",
      "228/228 [==============================] - 37s 160ms/step - loss: 1.1769\n",
      "Epoch 25/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1717\n",
      "Epoch 25: loss improved from 1.17691 to 1.17169, saving model to saved_weights/saved_weights-25-1.1717.hdf5\n",
      "228/228 [==============================] - 37s 160ms/step - loss: 1.1717\n",
      "Epoch 26/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1626\n",
      "Epoch 26: loss improved from 1.17169 to 1.16265, saving model to saved_weights/saved_weights-26-1.1626.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.1626\n",
      "Epoch 27/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1572\n",
      "Epoch 27: loss improved from 1.16265 to 1.15719, saving model to saved_weights/saved_weights-27-1.1572.hdf5\n",
      "228/228 [==============================] - 37s 160ms/step - loss: 1.1572\n",
      "Epoch 28/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1436\n",
      "Epoch 28: loss improved from 1.15719 to 1.14357, saving model to saved_weights/saved_weights-28-1.1436.hdf5\n",
      "228/228 [==============================] - 37s 164ms/step - loss: 1.1436\n",
      "Epoch 29/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1387\n",
      "Epoch 29: loss improved from 1.14357 to 1.13870, saving model to saved_weights/saved_weights-29-1.1387.hdf5\n",
      "228/228 [==============================] - 37s 162ms/step - loss: 1.1387\n",
      "Epoch 30/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1210\n",
      "Epoch 30: loss improved from 1.13870 to 1.12102, saving model to saved_weights/saved_weights-30-1.1210.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.1210\n",
      "Epoch 31/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1117\n",
      "Epoch 31: loss improved from 1.12102 to 1.11170, saving model to saved_weights/saved_weights-31-1.1117.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.1117\n",
      "Epoch 32/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0979\n",
      "Epoch 32: loss improved from 1.11170 to 1.09787, saving model to saved_weights/saved_weights-32-1.0979.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0979\n",
      "Epoch 33/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1045\n",
      "Epoch 33: loss did not improve from 1.09787\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.1045\n",
      "Epoch 34/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0848\n",
      "Epoch 34: loss improved from 1.09787 to 1.08479, saving model to saved_weights/saved_weights-34-1.0848.hdf5\n",
      "228/228 [==============================] - 36s 158ms/step - loss: 1.0848\n",
      "Epoch 35/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0788\n",
      "Epoch 35: loss improved from 1.08479 to 1.07881, saving model to saved_weights/saved_weights-35-1.0788.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0788\n",
      "Epoch 36/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0703\n",
      "Epoch 36: loss improved from 1.07881 to 1.07035, saving model to saved_weights/saved_weights-36-1.0703.hdf5\n",
      "228/228 [==============================] - 37s 160ms/step - loss: 1.0703\n",
      "Epoch 37/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0629\n",
      "Epoch 37: loss improved from 1.07035 to 1.06294, saving model to saved_weights/saved_weights-37-1.0629.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0629\n",
      "Epoch 38/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0521\n",
      "Epoch 38: loss improved from 1.06294 to 1.05211, saving model to saved_weights/saved_weights-38-1.0521.hdf5\n",
      "228/228 [==============================] - 37s 162ms/step - loss: 1.0521\n",
      "Epoch 39/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0552\n",
      "Epoch 39: loss did not improve from 1.05211\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0552\n",
      "Epoch 40/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0382\n",
      "Epoch 40: loss improved from 1.05211 to 1.03817, saving model to saved_weights/saved_weights-40-1.0382.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0382\n",
      "Epoch 41/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0294\n",
      "Epoch 41: loss improved from 1.03817 to 1.02942, saving model to saved_weights/saved_weights-41-1.0294.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0294\n",
      "Epoch 42/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0254\n",
      "Epoch 42: loss improved from 1.02942 to 1.02541, saving model to saved_weights/saved_weights-42-1.0254.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0254\n",
      "Epoch 43/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0150\n",
      "Epoch 43: loss improved from 1.02541 to 1.01496, saving model to saved_weights/saved_weights-43-1.0150.hdf5\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 1.0150\n",
      "Epoch 44/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0132\n",
      "Epoch 44: loss improved from 1.01496 to 1.01317, saving model to saved_weights/saved_weights-44-1.0132.hdf5\n",
      "228/228 [==============================] - 37s 163ms/step - loss: 1.0132\n",
      "Epoch 45/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0086\n",
      "Epoch 45: loss improved from 1.01317 to 1.00865, saving model to saved_weights/saved_weights-45-1.0086.hdf5\n",
      "228/228 [==============================] - 38s 166ms/step - loss: 1.0086\n",
      "Epoch 46/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9996\n",
      "Epoch 46: loss improved from 1.00865 to 0.99962, saving model to saved_weights/saved_weights-46-0.9996.hdf5\n",
      "228/228 [==============================] - 37s 162ms/step - loss: 0.9996\n",
      "Epoch 47/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9869\n",
      "Epoch 47: loss improved from 0.99962 to 0.98689, saving model to saved_weights/saved_weights-47-0.9869.hdf5\n",
      "228/228 [==============================] - 37s 162ms/step - loss: 0.9869\n",
      "Epoch 48/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9797\n",
      "Epoch 48: loss improved from 0.98689 to 0.97974, saving model to saved_weights/saved_weights-48-0.9797.hdf5\n",
      "228/228 [==============================] - 37s 162ms/step - loss: 0.9797\n",
      "Epoch 49/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9891\n",
      "Epoch 49: loss did not improve from 0.97974\n",
      "228/228 [==============================] - 37s 162ms/step - loss: 0.9891\n",
      "Epoch 50/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9677\n",
      "Epoch 50: loss improved from 0.97974 to 0.96774, saving model to saved_weights/saved_weights-50-0.9677.hdf5\n",
      "228/228 [==============================] - 37s 162ms/step - loss: 0.9677\n"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,   \n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "model.save('my_saved_weights_jungle_book_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8b45687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca2f98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate characters \n",
    "#We must provide a sequence of seq_lenth as input to start the generation process\n",
    "\n",
    "#The prediction results is probabilities for each of the 48 characters at a specific\n",
    "#point in sequence. Let us pick the one with max probability and print it out.\n",
    "#Writing our own softmax function....\n",
    "\n",
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29f4ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "# load the network weights\n",
    "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
    "model.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a753013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9599b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Seed for our text prediction: \"lia!\n",
      "see you on parade to-morrow, i suppose. good-night, old\"\n"
     ]
    }
   ],
   "source": [
    "#Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
    "#sys.stdout.write(generated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccf2afb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " be his fame. “shou prose to be splay, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/zmnwmmlj6d16wn3l4ql40ym80000gn/T/ipykernel_99388/3847965290.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seal where there was a gras sence make.,\n",
      "in a bun as not domp at lains, and when make he\n",
      "thim? he his hut by the boster socenty shark!” take\n",
      "i\n",
      "     here he had been saubbing in the nug,\n",
      "and the buffaloes, the buffaloes, and drupped his they get to snep, it was a farmse. “the man. what yer sure so but it man-under--if the sha suggeral of the jungle, of the moo\n"
     ]
    }
   ],
   "source": [
    "for i in range(400):   # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762a9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb0c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e2633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "230.852px",
    "left": "662px",
    "right": "20px",
    "top": "91px",
    "width": "380px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
