{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e6cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57155bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mowgli’s brothers\n",
      "\n",
      "     now rann the kite brings home the night\n",
      "        that mang the bat sets free--\n",
      "     the herds are shut in byre and hut\n",
      "        for loosed till dawn are we.\n",
      "     this is the hour of pride and power,\n",
      "        talon and tush and claw.\n",
      "     oh, hear the call!--good hunting all\n",
      "        that keep the jungle law!\n",
      "     night-song in the jungle\n",
      "\n",
      "it was seven o’clock of a very warm evening in the seeonee hills when\n",
      "father wolf woke up from his day’s rest, scratched himself, yawned, and\n",
      "spread out his paws one after the other to get rid of the sleepy feeling\n",
      "in their tips. mother wolf lay with her big gray nose dropped across her\n",
      "four tumbling, squealing cubs, and the moon shone into the mouth of the\n",
      "cave where they all lived. “augrh!” said father wolf. “it is time to\n",
      "hunt again.” he was going to spring down hill when a little shadow with\n",
      "a bushy tail crossed the threshold and whined: “good luck go with you, o\n",
      "chief of the wolves. and good luck and strong white teeth go with\n"
     ]
    }
   ],
   "source": [
    "#LOAD TEXT\n",
    "#Save notepad as UTF-8 (select from dropdown during saving)\n",
    "filename = \"data/book.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "print(raw_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e6539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TEXT\n",
    "#Remove digits\n",
    "raw_text = ''.join(c for c in raw_text if not c.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e015354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many total characters do we have in our training text?\n",
    "chars = sorted(list(set(raw_text))) #List of every character\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1bde73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters in the text; corpus length:  291857\n",
      "Total Vocab:  49\n"
     ]
    }
   ],
   "source": [
    "# summarize the data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters in the text; corpus length: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017aa702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 29180\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "#Now that we have characters we can create input/output sequences for training\n",
    "#Remember that for LSTM input and output can be sequences... hence the term seq2seq\n",
    "\n",
    "\n",
    "seq_length = 60  #Length of each input sequence\n",
    "step = 10   #Instead of moving 1 letter at a time, try skipping a few. \n",
    "sentences = []    # X values (Sentences)\n",
    "next_chars = []   # Y values. The character that follows the sentence defined as X\n",
    "for i in range(0, n_chars - seq_length, step):  #step=1 means each sentence is offset just by a single letter\n",
    "    sentences.append(raw_text[i: i + seq_length])  #Sequence in\n",
    "    next_chars.append(raw_text[i + seq_length])  #Sequence out\n",
    "n_patterns = len(sentences)    \n",
    "print('Number of sequences:', n_patterns)\n",
    "\n",
    "#Have a look at sentences and next_chars to see the continuity...\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080ab01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pk/zmnwmmlj6d16wn3l4ql40ym80000gn/T/ipykernel_99388/1466857379.py:13: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
      "/var/folders/pk/zmnwmmlj6d16wn3l4ql40ym80000gn/T/ipykernel_99388/1466857379.py:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29180, 60, 49)\n",
      "(29180, 49)\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False  True False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False  True False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False  True False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False  True False False False False False False False False False\n",
      "  False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "   True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False]]\n"
     ]
    }
   ],
   "source": [
    "#Just like time series, X is the sequence / sentence and y is the next value\n",
    "#that comes after the sentence... \n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "\n",
    "#time steps = sequence length\n",
    "#features = numbers of characters in our vocab (n_vocab)\n",
    "#Vectorize all sentences: there are n_patterns sentences.\n",
    "#For each sentence we have n_vocab characters available for seq_length\n",
    "#Vectorization returns a vector for all sentences indicating the presence or absence \n",
    "#of a character. \n",
    "\n",
    "x = np.zeros((len(sentences), seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), n_vocab), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[next_chars[i]]] = 1\n",
    "    \n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444856c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 11:37:10.564125: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               91136     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 49)                6321      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,457\n",
      "Trainable params: 97,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nostest/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Basic model with one LSTM\n",
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(seq_length, n_vocab)))\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b6789d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper model woth 2 LSTM\n",
    "#To stack LSTM layers, we need to change the configuration of the prior \n",
    "#LSTM layer to output a 3D array as input for the subsequent layer.\n",
    "#We can do this by setting the return_sequences argument on the layer to True \n",
    "#(defaults to False). This will return one output for each input time step and provide a 3D array.\n",
    "#Below is the same example as above with return_sequences=True.\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(128, input_shape=(seq_length, n_vocab), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(128))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "#optimizer = RMSprop(lr=0.01)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47908b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 2.2154\n",
      "Epoch 1: loss improved from inf to 2.21536, saving model to saved_weights/saved_weights-01-2.2154.hdf5\n",
      "228/228 [==============================] - 16s 69ms/step - loss: 2.2154\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.9476\n",
      "Epoch 2: loss improved from 2.21536 to 1.94756, saving model to saved_weights/saved_weights-02-1.9476.hdf5\n",
      "228/228 [==============================] - 17s 75ms/step - loss: 1.9476\n",
      "Epoch 3/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.7778\n",
      "Epoch 3: loss improved from 1.94756 to 1.77778, saving model to saved_weights/saved_weights-03-1.7778.hdf5\n",
      "228/228 [==============================] - 15s 67ms/step - loss: 1.7778\n",
      "Epoch 4/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.6479\n",
      "Epoch 4: loss improved from 1.77778 to 1.64789, saving model to saved_weights/saved_weights-04-1.6479.hdf5\n",
      "228/228 [==============================] - 14s 63ms/step - loss: 1.6479\n",
      "Epoch 5/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.5308\n",
      "Epoch 5: loss improved from 1.64789 to 1.53081, saving model to saved_weights/saved_weights-05-1.5308.hdf5\n",
      "228/228 [==============================] - 17s 73ms/step - loss: 1.5308\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 6: loss improved from 1.53081 to 1.43542, saving model to saved_weights/saved_weights-06-1.4354.hdf5\n",
      "228/228 [==============================] - 15s 67ms/step - loss: 1.4354\n",
      "Epoch 7/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.3437\n",
      "Epoch 7: loss improved from 1.43542 to 1.34373, saving model to saved_weights/saved_weights-07-1.3437.hdf5\n",
      "228/228 [==============================] - 15s 68ms/step - loss: 1.3437\n",
      "Epoch 8/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2669\n",
      "Epoch 8: loss improved from 1.34373 to 1.26691, saving model to saved_weights/saved_weights-08-1.2669.hdf5\n",
      "228/228 [==============================] - 15s 65ms/step - loss: 1.2669\n",
      "Epoch 9/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.2037\n",
      "Epoch 9: loss improved from 1.26691 to 1.20374, saving model to saved_weights/saved_weights-09-1.2037.hdf5\n",
      "228/228 [==============================] - 15s 64ms/step - loss: 1.2037\n",
      "Epoch 10/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1612\n",
      "Epoch 10: loss improved from 1.20374 to 1.16119, saving model to saved_weights/saved_weights-10-1.1612.hdf5\n",
      "228/228 [==============================] - 15s 64ms/step - loss: 1.1612\n",
      "Epoch 11/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.1131\n",
      "Epoch 11: loss improved from 1.16119 to 1.11309, saving model to saved_weights/saved_weights-11-1.1131.hdf5\n",
      "228/228 [==============================] - 15s 65ms/step - loss: 1.1131\n",
      "Epoch 12/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0795\n",
      "Epoch 12: loss improved from 1.11309 to 1.07954, saving model to saved_weights/saved_weights-12-1.0795.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 1.0795\n",
      "Epoch 13/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0507\n",
      "Epoch 13: loss improved from 1.07954 to 1.05071, saving model to saved_weights/saved_weights-13-1.0507.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 1.0507\n",
      "Epoch 14/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0248\n",
      "Epoch 14: loss improved from 1.05071 to 1.02476, saving model to saved_weights/saved_weights-14-1.0248.hdf5\n",
      "228/228 [==============================] - 14s 63ms/step - loss: 1.0248\n",
      "Epoch 15/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 1.0054\n",
      "Epoch 15: loss improved from 1.02476 to 1.00544, saving model to saved_weights/saved_weights-15-1.0054.hdf5\n",
      "228/228 [==============================] - 15s 64ms/step - loss: 1.0054\n",
      "Epoch 16/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9857\n",
      "Epoch 16: loss improved from 1.00544 to 0.98574, saving model to saved_weights/saved_weights-16-0.9857.hdf5\n",
      "228/228 [==============================] - 14s 63ms/step - loss: 0.9857\n",
      "Epoch 17/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9674\n",
      "Epoch 17: loss improved from 0.98574 to 0.96745, saving model to saved_weights/saved_weights-17-0.9674.hdf5\n",
      "228/228 [==============================] - 14s 63ms/step - loss: 0.9674\n",
      "Epoch 18/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9576\n",
      "Epoch 18: loss improved from 0.96745 to 0.95757, saving model to saved_weights/saved_weights-18-0.9576.hdf5\n",
      "228/228 [==============================] - 14s 63ms/step - loss: 0.9576\n",
      "Epoch 19/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9367\n",
      "Epoch 19: loss improved from 0.95757 to 0.93670, saving model to saved_weights/saved_weights-19-0.9367.hdf5\n",
      "228/228 [==============================] - 14s 63ms/step - loss: 0.9367\n",
      "Epoch 20/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9195\n",
      "Epoch 20: loss improved from 0.93670 to 0.91946, saving model to saved_weights/saved_weights-20-0.9195.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.9195\n",
      "Epoch 21/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.9062\n",
      "Epoch 21: loss improved from 0.91946 to 0.90622, saving model to saved_weights/saved_weights-21-0.9062.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.9062\n",
      "Epoch 22/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8902\n",
      "Epoch 22: loss improved from 0.90622 to 0.89019, saving model to saved_weights/saved_weights-22-0.8902.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.8902\n",
      "Epoch 23/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8841\n",
      "Epoch 23: loss improved from 0.89019 to 0.88412, saving model to saved_weights/saved_weights-23-0.8841.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.8841\n",
      "Epoch 24/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8744\n",
      "Epoch 24: loss improved from 0.88412 to 0.87439, saving model to saved_weights/saved_weights-24-0.8744.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.8744\n",
      "Epoch 25/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8548\n",
      "Epoch 25: loss improved from 0.87439 to 0.85481, saving model to saved_weights/saved_weights-25-0.8548.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.8548\n",
      "Epoch 26/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8401\n",
      "Epoch 26: loss improved from 0.85481 to 0.84007, saving model to saved_weights/saved_weights-26-0.8401.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.8401\n",
      "Epoch 27/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8406\n",
      "Epoch 27: loss did not improve from 0.84007\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.8406\n",
      "Epoch 28/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8237\n",
      "Epoch 28: loss improved from 0.84007 to 0.82373, saving model to saved_weights/saved_weights-28-0.8237.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.8237\n",
      "Epoch 29/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8183\n",
      "Epoch 29: loss improved from 0.82373 to 0.81829, saving model to saved_weights/saved_weights-29-0.8183.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.8183\n",
      "Epoch 30/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8035\n",
      "Epoch 30: loss improved from 0.81829 to 0.80354, saving model to saved_weights/saved_weights-30-0.8035.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.8035\n",
      "Epoch 31/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.8012\n",
      "Epoch 31: loss improved from 0.80354 to 0.80124, saving model to saved_weights/saved_weights-31-0.8012.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.8012\n",
      "Epoch 32/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7875\n",
      "Epoch 32: loss improved from 0.80124 to 0.78749, saving model to saved_weights/saved_weights-32-0.7875.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.7875\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - ETA: 0s - loss: 0.7831\n",
      "Epoch 33: loss improved from 0.78749 to 0.78306, saving model to saved_weights/saved_weights-33-0.7831.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.7831\n",
      "Epoch 34/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7691\n",
      "Epoch 34: loss improved from 0.78306 to 0.76910, saving model to saved_weights/saved_weights-34-0.7691.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.7691\n",
      "Epoch 35/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7538\n",
      "Epoch 35: loss improved from 0.76910 to 0.75384, saving model to saved_weights/saved_weights-35-0.7538.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.7538\n",
      "Epoch 36/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7480\n",
      "Epoch 36: loss improved from 0.75384 to 0.74797, saving model to saved_weights/saved_weights-36-0.7480.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.7480\n",
      "Epoch 37/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7461\n",
      "Epoch 37: loss improved from 0.74797 to 0.74612, saving model to saved_weights/saved_weights-37-0.7461.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.7461\n",
      "Epoch 38/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7288\n",
      "Epoch 38: loss improved from 0.74612 to 0.72880, saving model to saved_weights/saved_weights-38-0.7288.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.7288\n",
      "Epoch 39/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7204\n",
      "Epoch 39: loss improved from 0.72880 to 0.72036, saving model to saved_weights/saved_weights-39-0.7204.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.7204\n",
      "Epoch 40/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7111\n",
      "Epoch 40: loss improved from 0.72036 to 0.71113, saving model to saved_weights/saved_weights-40-0.7111.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.7111\n",
      "Epoch 41/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.7060\n",
      "Epoch 41: loss improved from 0.71113 to 0.70597, saving model to saved_weights/saved_weights-41-0.7060.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.7060\n",
      "Epoch 42/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6931\n",
      "Epoch 42: loss improved from 0.70597 to 0.69311, saving model to saved_weights/saved_weights-42-0.6931.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.6931\n",
      "Epoch 43/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6906\n",
      "Epoch 43: loss improved from 0.69311 to 0.69058, saving model to saved_weights/saved_weights-43-0.6906.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.6906\n",
      "Epoch 44/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6916\n",
      "Epoch 44: loss did not improve from 0.69058\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.6916\n",
      "Epoch 45/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6756\n",
      "Epoch 45: loss improved from 0.69058 to 0.67563, saving model to saved_weights/saved_weights-45-0.6756.hdf5\n",
      "228/228 [==============================] - 14s 61ms/step - loss: 0.6756\n",
      "Epoch 46/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6667\n",
      "Epoch 46: loss improved from 0.67563 to 0.66673, saving model to saved_weights/saved_weights-46-0.6667.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.6667\n",
      "Epoch 47/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6589\n",
      "Epoch 47: loss improved from 0.66673 to 0.65894, saving model to saved_weights/saved_weights-47-0.6589.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.6589\n",
      "Epoch 48/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6537\n",
      "Epoch 48: loss improved from 0.65894 to 0.65374, saving model to saved_weights/saved_weights-48-0.6537.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.6537\n",
      "Epoch 49/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6529\n",
      "Epoch 49: loss improved from 0.65374 to 0.65292, saving model to saved_weights/saved_weights-49-0.6529.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.6529\n",
      "Epoch 50/50\n",
      "228/228 [==============================] - ETA: 0s - loss: 0.6399\n",
      "Epoch 50: loss improved from 0.65292 to 0.63989, saving model to saved_weights/saved_weights-50-0.6399.hdf5\n",
      "228/228 [==============================] - 14s 62ms/step - loss: 0.6399\n"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"saved_weights/saved_weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "history = model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=50,   \n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "model.save('my_saved_weights_jungle_book_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b45687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca2f98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate characters \n",
    "#We must provide a sequence of seq_lenth as input to start the generation process\n",
    "\n",
    "#The prediction results is probabilities for each of the 48 characters at a specific\n",
    "#point in sequence. Let us pick the one with max probability and print it out.\n",
    "#Writing our own softmax function....\n",
    "\n",
    "def sample(preds):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds) #exp of log (x), isn't this same as x??\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1) \n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29f4ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "# load the network weights\n",
    "filename = \"my_saved_weights_jungle_book_50epochs.h5\"\n",
    "model.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a753013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a random sentence from the text as seed.\n",
    "start_index = random.randint(0, n_chars - seq_length - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9599b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Seed for our text prediction: \"le the guns are being put together, and\n",
      "then you watch the l\"\n"
     ]
    }
   ],
   "source": [
    "#Initiate generated text and keep adding new predictions and print them out\n",
    "generated = ''\n",
    "sentence = raw_text[start_index: start_index + seq_length]\n",
    "generated += sentence\n",
    "\n",
    "print('----- Seed for our text prediction: \"' + sentence + '\"')\n",
    "#sys.stdout.write(generated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8a34280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illlack shim there was a manought behind. “gooke-teely belowehs! you pricked fighting, moon, and weaks as sloeted and still and back on a shoulder came thee calls\n",
      "and scat for\n",
      "him for a when the seowed in the sloplapes we cale folling sea hicke and play, slow slad pack, and ape stillound.\n",
      "\n",
      "“allow horse that most after was long mostly as a ling it if not or this work wort or him, who once the last \n"
     ]
    }
   ],
   "source": [
    "for i in range(400):   # Number of characters including spaces\n",
    "    x_pred = np.zeros((1, seq_length, n_vocab))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = int_to_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762a9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb0c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052dab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e2633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "230.852px",
    "left": "662px",
    "right": "20px",
    "top": "91px",
    "width": "380px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
